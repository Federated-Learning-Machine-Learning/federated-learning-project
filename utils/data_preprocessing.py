# -*- coding: utf-8 -*-
"""data preproccesing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zdx3XwuvkbB53jOFV2hQuYJu3HotjSdc
"""

import torch
from torch.utils.data import random_split
from torchvision.datasets import CIFAR100
from torchvision import transforms
from torchvision.transforms import AutoAugment, AutoAugmentPolicy


class CIFAR100Pipeline:
    """
    Dataset pipeline class for CIFAR-100.
    Handles DINO ViT-compatible preprocessing and splitting into train/val/test sets.
    """

    def __init__(self, data_dir: str = "./data", val_split: float = 0.1, use_augment: bool = True):
        """
        Initialize the dataset pipeline.

        Args:
            data_dir (str): Directory to store/load CIFAR-100.
            val_split (float): Proportion of training set used for validation.
            use_augment (bool): Whether to use AutoAugment in training transforms.
        """
        self.data_dir = data_dir
        self.val_split = val_split
        self.use_augment = use_augment

        # ImageNet normalization (DINO compatible)
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]

        # Define transforms
        self.train_transform = self._build_train_transform()
        self.test_transform = self._build_test_transform()

    def _build_train_transform(self):
        """Build DINO-style training transform with optional AutoAugment."""
        transform_list = [
            transforms.Resize(256),
            transforms.RandomCrop(224),
            transforms.RandomHorizontalFlip()
        ]

        if self.use_augment:
            transform_list.append(AutoAugment(policy=AutoAugmentPolicy.CIFAR10))

        transform_list.extend([
            transforms.ToTensor(),
            transforms.Normalize(mean=self.mean, std=self.std),
        ])

        return transforms.Compose(transform_list)

    def _build_test_transform(self):
        """Build standard test/validation transform (no augmentation)."""
        return transforms.Compose([
            transforms.Resize(224),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=self.mean, std=self.std),
        ])

    def run_pipeline(self):
        """
        Load CIFAR-100 dataset and return train, val, and test sets.

        Returns:
            Tuple[Dataset, Dataset, Dataset]: trainset, valset, testset
        """
        # Load full train set with train transform
        full_trainset = CIFAR100(
            root=self.data_dir,
            train=True,
            transform=self.train_transform,
            download=True
        )

        # Split into train/val
        val_size = int(len(full_trainset) * self.val_split)
        train_size = len(full_trainset) - val_size
        trainset, valset = random_split(full_trainset, [train_size, val_size])

        # Overwrite val transform (remove augmentations)
        valset.dataset.transform = self.test_transform

        # Load test set
        testset = CIFAR100(
            root=self.data_dir,
            train=False,
            transform=self.test_transform,
            download=True
        )

        return trainset, valset, testset
