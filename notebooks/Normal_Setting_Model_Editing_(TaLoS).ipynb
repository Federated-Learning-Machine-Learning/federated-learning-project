{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th-4tlWzbEBg",
        "outputId": "c3c665e9-ba29-4327-8dd1-de3186a8d486",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm torchvision\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import timm\n",
        "from timm.loss import LabelSmoothingCrossEntropy\n",
        "from timm.models.layers import DropPath\n",
        "import data_preprocessing\n",
        "from wandb_logger import WandBLogger\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "uZL36JvVBqdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login 89e5fee022a3a1cf86f958ee0b3dff6f2aa57aad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBxCiv9euzHx",
        "outputId": "33be1f29-99ef-4468-f51e-867d8d99b7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME = \"federated-learning-project\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "VAL_SPLIT = 0.1\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "pB6ym65EBtJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibility\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgXa-T1_BvQu",
        "outputId": "1b01685d-fdde-4427-b150-4d4644ae3f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c0e38d6baf0>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = data_preprocessing.CIFAR100Pipeline(val_split=VAL_SPLIT, use_augment=True)\n",
        "trainset, valset, testset = pipeline.run_pipeline()"
      ],
      "metadata": {
        "id": "NTdhFSHnBxbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=BATCH_SIZE)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "lm1uFdpCBzNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "def create_dino_vit_s16_for_cifar100(freezing=True):\n",
        "    model = timm.create_model(\"vit_small_patch16_224_dino\", pretrained=True, num_classes=0)\n",
        "\n",
        "    # Replace the head with CIFAR-100 classification head\n",
        "    model.head = nn.Linear(model.num_features, 100)\n",
        "\n",
        "    if freezing:\n",
        "      # Freeze all parameters except head\n",
        "      for param in model.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "      # Unfreeze only the head\n",
        "      for param in model.head.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_dino_vit_s16_for_cifar100(False).to(DEVICE)"
      ],
      "metadata": {
        "id": "7pIhMsouB0r2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6c2b47-95f0-4867-c8d2-5367600f197e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name vit_small_patch16_224_dino to current vit_small_patch16_224.dino.\n",
            "  model = create_fn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(next(model.parameters()).device)\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {trainable:,} / {total:,}\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmfmRkXzB3oV",
        "outputId": "e22541dd-e972-4d81-99f3-f4168787b21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Trainable params: 21,704,164 / 21,704,164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD\n",
        "\n",
        "class SparseSGDM(SGD):\n",
        "    def __init__(self, params, lr, momentum=0, weight_decay=0, masks=None):\n",
        "        \"\"\"\n",
        "        params: Model parameters to update.\n",
        "        lr: Learning rate.\n",
        "        momentum: Momentum factor.\n",
        "        weight_decay: Weight decay (L2 penalty).\n",
        "        masks: List of binary masks matching the shape of model parameters.\n",
        "        \"\"\"\n",
        "        super().__init__(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        self.masks = masks\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            for i, p in enumerate(group['params']):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                # Apply mask if available\n",
        "                if self.masks is not None:\n",
        "                    mask = self.masks[i]\n",
        "                    p.grad.data.mul_(mask.to(p.grad.device))\n",
        "        super().step(closure)\n"
      ],
      "metadata": {
        "id": "REzbU5jmB4C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"def compute_fisher_diag(model, dataloader, criterion, device, num_batches=1):\n",
        "    model.eval()\n",
        "    fisher_diagonal = [torch.zeros_like(p, device=device) for p in model.head.parameters()]\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(dataloader):\n",
        "        if batch_idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        model.zero_grad()\n",
        "\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "\n",
        "        for i, p in enumerate(model.head.parameters()):\n",
        "            if p.grad is not None:\n",
        "                fisher_diagonal[i] += (p.grad.data ** 2) * x.size(0)\n",
        "\n",
        "        total_samples += x.size(0)\n",
        "\n",
        "    for i in range(len(fisher_diagonal)):\n",
        "        fisher_diagonal[i] /= total_samples\n",
        "\n",
        "    return fisher_diagonal\n",
        "\n",
        "def fisher_mask_from_diag(fisher_diag, sparsity=0.5):\n",
        "    masks = []\n",
        "    for f in fisher_diag:\n",
        "        threshold = torch.quantile(f.flatten(), sparsity)\n",
        "        mask = (f >= threshold).float()\n",
        "        masks.append(mask)\n",
        "    return masks\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"# Compute Fisher diagonal and generate masks\n",
        "fisher_diag = compute_fisher_diag(model, valloader, criterion, DEVICE, num_batches=3)\n",
        "masks = fisher_mask_from_diag(fisher_diag, sparsity=0.2)\"\"\""
      ],
      "metadata": {
        "id": "zScc04RSWCEO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7e78c114-4e07-4f42-d257-b7ded7792983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Compute Fisher diagonal and generate masks\\nfisher_diag = compute_fisher_diag(model, valloader, criterion, DEVICE, num_batches=3)\\nmasks = fisher_mask_from_diag(fisher_diag, sparsity=0.2)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TaLoSPruner:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.scores = {}\n",
        "\n",
        "        # Verificare la struttura del modello\n",
        "        if hasattr(model, 'head'):\n",
        "            self.head = model.head\n",
        "        else:\n",
        "            print(\"Attenzione: il modello non ha un attributo 'head'. Utilizzo l'intero modello.\")\n",
        "            self.head = model\n",
        "\n",
        "    def score(self, dataloader, num_batches=1):\n",
        "        self.model.eval()\n",
        "\n",
        "        # Inizializza dizionario dei punteggi\n",
        "        for p in self.head.parameters():\n",
        "            self.scores[p] = torch.zeros_like(p, device=self.device)\n",
        "\n",
        "        total_samples = 0\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(dataloader):\n",
        "            if batch_idx >= num_batches:\n",
        "                break\n",
        "\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            # Forward pass con gestione di diversi tipi di modelli\n",
        "            if hasattr(self.model, 'forward_features'):\n",
        "                with torch.no_grad():\n",
        "                    features = self.model.forward_features(x)\n",
        "\n",
        "                # Gestisci diverse strutture di output\n",
        "                if features.ndim == 3:  # [B, seq_len, hidden_dim]\n",
        "                    cls_token = features[:, 0, :]  # Prendi solo il CLS token\n",
        "                else:\n",
        "                    cls_token = features  # Già nella forma corretta\n",
        "\n",
        "                # Forward pass solo sulla testa\n",
        "                output = self.head(cls_token)\n",
        "            else:\n",
        "                # Se il modello non ha forward_features, usa il forward normale\n",
        "                self.model.zero_grad()\n",
        "                output = self.model(x)\n",
        "\n",
        "            # Adatta l'output se necessario\n",
        "            if output.ndim == 3:\n",
        "                output = output.squeeze(1)\n",
        "\n",
        "            # Calcolo della loss\n",
        "            loss = nn.CrossEntropyLoss()(output, y)\n",
        "\n",
        "            # Backward per calcolare i gradienti\n",
        "            loss.backward()\n",
        "\n",
        "            # Accumula i punteggi (Fisher diagonale approssimato)\n",
        "            for p in self.head.parameters():\n",
        "                if p.grad is not None:\n",
        "                    self.scores[p] += (p.grad.data ** 2) * x.size(0)\n",
        "\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "        # Normalizza i punteggi\n",
        "        for p in self.head.parameters():\n",
        "            if p in self.scores and total_samples > 0:\n",
        "                self.scores[p] /= total_samples\n",
        "\n",
        "\n",
        "    \"\"\"def generate_masks(self, sparsity=0.5):\n",
        "        masks = []\n",
        "        for p in self.model.parameters():\n",
        "            if p in self.scores:\n",
        "                score = self.scores[p]\n",
        "                threshold = torch.quantile(score.flatten(), sparsity)\n",
        "                mask = (score >= threshold).float()\n",
        "                masks.append(mask)\n",
        "            else:\n",
        "                # Parametri non prunati (ad esempio il backbone)\n",
        "                masks.append(torch.ones_like(p, device=self.device))\n",
        "        return masks\"\"\"\n",
        "\n",
        "    def generate_masks(self, sparsity=0.5):\n",
        "        masks = []\n",
        "\n",
        "        for p in self.head.parameters():\n",
        "            if p in self.scores:\n",
        "                score = self.scores[p]\n",
        "\n",
        "                # Assicurati che lo score non sia completamente zero\n",
        "                if torch.all(score == 0):\n",
        "                    print(f\"Attenzione: tutti gli score sono zero per un parametro di forma {p.shape}!\")\n",
        "                    masks.append(torch.ones_like(p, device=self.device))\n",
        "                    continue\n",
        "\n",
        "                # Calcola soglia e maschera\n",
        "                threshold = torch.quantile(score.flatten(), sparsity)\n",
        "                mask = (score >= threshold).float()\n",
        "\n",
        "                # Verifica che non stiamo eliminando troppi pesi\n",
        "                keep_percent = mask.sum() / mask.numel()\n",
        "                if keep_percent < 0.05:  # Mantieni almeno il 5% dei pesi\n",
        "                    print(f\"Attenzione: stai mantenendo solo {keep_percent:.2%} dei pesi! Regolando...\")\n",
        "                    # Riduci la sparsità per mantenere più pesi\n",
        "                    top_k = max(int(0.05 * mask.numel()), 1)\n",
        "                    values, _ = torch.topk(score.flatten(), top_k)\n",
        "                    threshold = values.min()\n",
        "                    mask = (score >= threshold).float()\n",
        "\n",
        "                masks.append(mask)\n",
        "\n",
        "        return masks\n"
      ],
      "metadata": {
        "id": "Ar4b0X8HcA3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_pruning(pruner, dataloader, rounds=4, final_sparsity=0.9, num_batches=3):\n",
        "    # La sparsità target finale è la frazione di pesi da rimuovere\n",
        "    keep_ratio = 1.0 - final_sparsity\n",
        "\n",
        "    for r in range(rounds):\n",
        "        # Sparsità intermedia (cresce progressivamente)\n",
        "        current_keep = keep_ratio ** ((r + 1) / rounds)\n",
        "        current_sparsity = 1.0 - current_keep\n",
        "        print(f\"[Round {r+1}/{rounds}] Target sparsity: {current_sparsity:.4f}\")\n",
        "\n",
        "        # Calcolo dello score basato sui gradienti (Fisher)\n",
        "        pruner.score(dataloader, num_batches=num_batches)\n",
        "\n",
        "        # Calcolo della nuova maschera\n",
        "        masks = pruner.generate_masks(sparsity=current_sparsity)\n",
        "\n",
        "    return masks"
      ],
      "metadata": {
        "id": "I29kThG7_QiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = WandBLogger(\n",
        "    project_name=PROJECT_NAME,\n",
        "    run_name=\"CENTRALIZED MODEL EDITING (Talos calibrating)-Run-1\",\n",
        "    config={\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"SparseSGDM\",\n",
        "        \"scheduler\": \"CosineAnnealing + Warmup\",\n",
        "        \"weight_decay\": 0.0005,\n",
        "        \"sparsity\": 0.6,\n",
        "        \"calibration_rounds\": 4\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "pWSKG3rv5XyD",
        "outputId": "7b02137a-e560-4ffb-8586-a543a311dee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>▁▄█</td></tr><tr><td>train_acc</td><td>▁▂█</td></tr><tr><td>train_loss</td><td>█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>3e-05</td></tr><tr><td>train_acc</td><td>0.04789</td></tr><tr><td>train_loss</td><td>4.74213</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CENTRALIZED MODEL EDITING (Talos calibrating)-Run-1</strong> at: <a href='https://wandb.ai/polito-fl/federated-learning-project/runs/mk0ngqjk' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project/runs/mk0ngqjk</a><br> View project at: <a href='https://wandb.ai/polito-fl/federated-learning-project' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_134607-mk0ngqjk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_141008-ug35db5h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/polito-fl/federated-learning-project/runs/ug35db5h' target=\"_blank\">CENTRALIZED MODEL EDITING (Talos calibrating)-Run-1</a></strong> to <a href='https://wandb.ai/polito-fl/federated-learning-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/polito-fl/federated-learning-project' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/polito-fl/federated-learning-project/runs/ug35db5h' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project/runs/ug35db5h</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "# Initialize components\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Setup pruning con parametri migliorati\n",
        "pruner = TaLoSPruner(model, device=DEVICE)\n",
        "\n",
        "# Utilizzo una sparsità meno aggressiva (50% invece di 60%)\n",
        "# e più batch per una valutazione più affidabile\n",
        "masks = iterative_pruning(pruner, valloader, rounds=4, final_sparsity=0.5, num_batches=10)\n",
        "\n",
        "# Verifica delle maschere prima di utilizzarle\n",
        "print(f\"Numero di maschere generate: {len(masks)}\")\n",
        "for i, mask in enumerate(masks):\n",
        "    keep_percent = mask.sum() / mask.numel()\n",
        "    print(f\"Maschera #{i}: shape={mask.shape}, mantiene {keep_percent:.2%} dei pesi\")\n",
        "\n",
        "# Initialize optimizer with masks\n",
        "optimizer = SparseSGDM(\n",
        "    model.head.parameters(),\n",
        "    lr=5e-5,  # Learning rate di base invariato\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005,\n",
        "    masks=masks\n",
        ")\n",
        "\n",
        "# Scheduler with warmup + cosine - correzione del fattore di start\n",
        "warmup_epochs = 5\n",
        "cosine_epochs = EPOCHS - warmup_epochs\n",
        "\n",
        "scheduler = optim.lr_scheduler.SequentialLR(\n",
        "    optimizer,\n",
        "    schedulers=[\n",
        "        # Corretto il fattore iniziale per un warm-up più efficace\n",
        "        optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup_epochs),\n",
        "        optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cosine_epochs)\n",
        "    ],\n",
        "    milestones=[warmup_epochs]\n",
        ")\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 6\n",
        "best_val_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    correct, total, train_loss = 0, 0, 0.0\n",
        "\n",
        "    for x, y in trainloader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Applicazione maschere con controllo migliore della forma\n",
        "        for i, (p, mask) in enumerate(zip(model.head.parameters(), masks)):\n",
        "            # Verifica compatibilità di forma prima di applicare\n",
        "            if p.shape == mask.shape:\n",
        "                p.data.mul_(mask.to(p.device))  # Zero out pruned weights\n",
        "            else:\n",
        "                # Tenta l'adattamento della maschera se possibile\n",
        "                try:\n",
        "                    # Se la maschera ha dimensioni extra, ridimensionala\n",
        "                    reshaped_mask = mask\n",
        "                    while reshaped_mask.dim() > p.dim() and reshaped_mask.shape[0] == 1:\n",
        "                        reshaped_mask = reshaped_mask.squeeze(0)\n",
        "\n",
        "                    # Se serve un'espansione\n",
        "                    if reshaped_mask.dim() < p.dim():\n",
        "                        for _ in range(p.dim() - reshaped_mask.dim()):\n",
        "                            reshaped_mask = reshaped_mask.unsqueeze(0)\n",
        "\n",
        "                    # Verifica finale\n",
        "                    if p.shape == reshaped_mask.shape:\n",
        "                        p.data.mul_(reshaped_mask.to(p.device))\n",
        "                    else:\n",
        "                        print(f\"Warning [Epoch {epoch+1}]: Impossibile applicare maschera #{i}. \"\n",
        "                              f\"Param shape {p.shape}, Mask shape {mask.shape}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Errore applicazione maschera #{i}: {str(e)}\")\n",
        "\n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_acc = correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "\n",
        "     # Validation\n",
        "    model.eval()\n",
        "    correct, total, val_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in valloader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            val_loss += loss.item() * y.size(0)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    val_acc = correct / total\n",
        "    val_loss /= total\n",
        "\n",
        "    # Aggiorna il log con i risultati della validazione\n",
        "    logger.log_metrics({\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_acc\": val_acc\n",
        "    }, step=epoch)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{EPOCHS} — Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "    # Early stopping logic con dettagli aggiuntivi\n",
        "    if val_acc > best_val_acc:\n",
        "        improvement = val_acc - best_val_acc\n",
        "        best_val_acc = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        best_model_state = model.state_dict()  # Save best model\n",
        "        logger.log_model(model, path=\"best_model.pth\")\n",
        "        print(f\"Miglioramento dell'accuratezza: +{improvement:.4f}. Modello salvato.\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"Nessun miglioramento per {epochs_no_improve}/{patience} epoche.\")\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggeerato all'epoca {epoch+1}. Migliore accuratezza: {best_val_acc:.4f}\")\n",
        "            break\n",
        "\n",
        "# Ripristina il miglior modello alla fine del training\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f\"Training completato. Ripristinato il miglior modello con accuratezza: {best_val_acc:.4f}\")\n",
        "\n",
        "# Stampa riassunto finale\n",
        "print(\"\\n==== RIASSUNTO TRAINING ====\")\n",
        "print(f\"Migliore accuratezza validazione: {best_val_acc:.4f}\")\n",
        "print(f\"Sparsità finale della testa: {1.0 - sum(m.sum() for m in masks) / sum(m.numel() for m in masks):.2%}\")\n",
        "print(\"============================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWgY0_ppIVGH",
        "outputId": "f8a8f769-f682-4c0a-a1c9-09293f845f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-122-22be8701be36>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 1/4] Target sparsity: 0.1591\n",
            "[Round 2/4] Target sparsity: 0.2929\n",
            "[Round 3/4] Target sparsity: 0.4054\n",
            "[Round 4/4] Target sparsity: 0.5000\n",
            "Numero di maschere generate: 2\n",
            "Maschera #0: shape=torch.Size([100, 384]), mantiene 50.00% dei pesi\n",
            "Maschera #1: shape=torch.Size([100]), mantiene 50.00% dei pesi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-122-22be8701be36>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/20 — Train Acc: 0.0091 | Val Acc: 0.0144 | LR: 0.000014\n",
            "Miglioramento dell'accuratezza: +0.0144. Modello salvato.\n",
            "Epoch 02/20 — Train Acc: 0.0297 | Val Acc: 0.0494 | LR: 0.000023\n",
            "Miglioramento dell'accuratezza: +0.0350. Modello salvato.\n",
            "Epoch 03/20 — Train Acc: 0.0910 | Val Acc: 0.1420 | LR: 0.000032\n",
            "Miglioramento dell'accuratezza: +0.0926. Modello salvato.\n",
            "Epoch 04/20 — Train Acc: 0.2104 | Val Acc: 0.2726 | LR: 0.000041\n",
            "Miglioramento dell'accuratezza: +0.1306. Modello salvato.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05/20 — Train Acc: 0.3306 | Val Acc: 0.3762 | LR: 0.000050\n",
            "Miglioramento dell'accuratezza: +0.1036. Modello salvato.\n",
            "Epoch 06/20 — Train Acc: 0.4233 | Val Acc: 0.4534 | LR: 0.000049\n",
            "Miglioramento dell'accuratezza: +0.0772. Modello salvato.\n",
            "Epoch 07/20 — Train Acc: 0.4833 | Val Acc: 0.5006 | LR: 0.000048\n",
            "Miglioramento dell'accuratezza: +0.0472. Modello salvato.\n",
            "Epoch 08/20 — Train Acc: 0.5225 | Val Acc: 0.5328 | LR: 0.000045\n",
            "Miglioramento dell'accuratezza: +0.0322. Modello salvato.\n",
            "Epoch 09/20 — Train Acc: 0.5520 | Val Acc: 0.5520 | LR: 0.000042\n",
            "Miglioramento dell'accuratezza: +0.0192. Modello salvato.\n",
            "Epoch 10/20 — Train Acc: 0.5727 | Val Acc: 0.5664 | LR: 0.000038\n",
            "Miglioramento dell'accuratezza: +0.0144. Modello salvato.\n",
            "Epoch 11/20 — Train Acc: 0.5863 | Val Acc: 0.5768 | LR: 0.000033\n",
            "Miglioramento dell'accuratezza: +0.0104. Modello salvato.\n",
            "Epoch 12/20 — Train Acc: 0.5974 | Val Acc: 0.5856 | LR: 0.000028\n",
            "Miglioramento dell'accuratezza: +0.0088. Modello salvato.\n",
            "Epoch 13/20 — Train Acc: 0.6066 | Val Acc: 0.5948 | LR: 0.000022\n",
            "Miglioramento dell'accuratezza: +0.0092. Modello salvato.\n",
            "Epoch 14/20 — Train Acc: 0.6128 | Val Acc: 0.5984 | LR: 0.000017\n",
            "Miglioramento dell'accuratezza: +0.0036. Modello salvato.\n",
            "Epoch 15/20 — Train Acc: 0.6182 | Val Acc: 0.6050 | LR: 0.000013\n",
            "Miglioramento dell'accuratezza: +0.0066. Modello salvato.\n",
            "Epoch 16/20 — Train Acc: 0.6211 | Val Acc: 0.6074 | LR: 0.000008\n",
            "Miglioramento dell'accuratezza: +0.0024. Modello salvato.\n",
            "Epoch 17/20 — Train Acc: 0.6234 | Val Acc: 0.6080 | LR: 0.000005\n",
            "Miglioramento dell'accuratezza: +0.0006. Modello salvato.\n",
            "Epoch 18/20 — Train Acc: 0.6251 | Val Acc: 0.6076 | LR: 0.000002\n",
            "Nessun miglioramento per 1/6 epoche.\n",
            "Epoch 19/20 — Train Acc: 0.6257 | Val Acc: 0.6080 | LR: 0.000001\n",
            "Nessun miglioramento per 2/6 epoche.\n",
            "Epoch 20/20 — Train Acc: 0.6257 | Val Acc: 0.6080 | LR: 0.000000\n",
            "Nessun miglioramento per 3/6 epoche.\n",
            "Training completato. Ripristinato il miglior modello con accuratezza: 0.6080\n",
            "\n",
            "==== RIASSUNTO TRAINING ====\n",
            "Migliore accuratezza validazione: 0.6080\n",
            "Sparsità finale della testa: 50.00%\n",
            "============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Test evaluation\n",
        "model.eval()\n",
        "correct, total, test_loss = 0, 0, 0.0\n",
        "with torch.no_grad():\n",
        "    for x, y in testloader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "\n",
        "        test_loss += loss.item() * y.size(0)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "test_acc = correct / total\n",
        "test_loss /= total\n",
        "\n",
        "logger.log_metrics({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_acc\": test_acc\n",
        "})\n",
        "\n",
        "logger.finish()\n",
        "\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "pdEKpAqhWLyY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "3946797d-3be5-48a8-b1dc-a3afd96d5fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>val_acc</td><td>▁▁▃▄▅▆▇▇▇███████████</td></tr><tr><td>val_loss</td><td>█▆▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>0.6052</td></tr><tr><td>test_loss</td><td>2.08908</td></tr><tr><td>val_acc</td><td>0.608</td></tr><tr><td>val_loss</td><td>2.06817</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CENTRALIZED MODEL EDITING (Talos calibrating)-Run-1</strong> at: <a href='https://wandb.ai/polito-fl/federated-learning-project/runs/ug35db5h' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project/runs/ug35db5h</a><br> View project at: <a href='https://wandb.ai/polito-fl/federated-learning-project' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_141008-ug35db5h/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Accuracy: 0.6052 | Test Loss: 2.0891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#!pip install ray[tune]\n",
        "from ray import tune\n",
        "\n",
        "search_space = {\n",
        "    \"lr\": tune.grid_search([0.01, 0.1]),\n",
        "    \"momentum\": 0.9,\n",
        "    \"weight_decay\": 5e-4,\n",
        "    \"batch_size\": 128,\n",
        "    \"sparsity\": tune.grid_search([0.2, 0.5, 0.8])  # proporzione di pesi NON aggiornati\n",
        "}\"\"\""
      ],
      "metadata": {
        "id": "A1wey3tM0fw4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9eaa4084-ab83-4bc8-c3be-f096a8d3e554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#!pip install ray[tune]\\nfrom ray import tune\\n\\nsearch_space = {\\n    \"lr\": tune.grid_search([0.01, 0.1]),\\n    \"momentum\": 0.9,\\n    \"weight_decay\": 5e-4,\\n    \"batch_size\": 128,\\n    \"sparsity\": tune.grid_search([0.2, 0.5, 0.8])  # proporzione di pesi NON aggiornati\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from torchvision import transforms\n",
        "\n",
        "def get_cifar_transform() -> transforms.Compose:\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize CIFAR images to 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
        "            std=[0.229, 0.224, 0.225]    # ImageNet stds\n",
        "        )\n",
        "    ])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FaH0bPkK0gdE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a88db332-3659-46e2-fbc2-66597b63b661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from torchvision import transforms\\n\\ndef get_cifar_transform() -> transforms.Compose:\\n    return transforms.Compose([\\n        transforms.Resize((224, 224)),  # Resize CIFAR images to 224x224\\n        transforms.ToTensor(),\\n        transforms.Normalize(\\n            mean=[0.485, 0.456, 0.406],  # ImageNet means\\n            std=[0.229, 0.224, 0.225]    # ImageNet stds\\n        )\\n    ])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from ray.train import Checkpoint\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from ray.train import report\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import shutil\n",
        "\n",
        "def train_vit(config):\n",
        "    # 1. Crea il modello\n",
        "    model = create_dino_vit_s16_for_cifar100().to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # 2. Prepara i dati\n",
        "    pipeline = data_preprocessing.CIFAR100Pipeline(val_split=VAL_SPLIT, use_augment=True)\n",
        "    trainset, valset, testset = pipeline.run_pipeline()\n",
        "    trainloader = DataLoader(trainset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    valloader = DataLoader(valset, batch_size=config[\"batch_size\"])\n",
        "    testloader = DataLoader(testset, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    # 3. Calcola Fisher diagonal e maschera\n",
        "    fisher_diag = compute_fisher_diag(model, valloader, criterion, DEVICE, num_batches=3)\n",
        "    masks = fisher_mask_from_diag(fisher_diag, sparsity=config[\"sparsity\"])\n",
        "\n",
        "    # --- CALCOLO SCORE & MASCHERE CON IL TUO TALOSPRUNER ---\n",
        "    pruner = TaLoSPruner(model.head, device=DEVICE)\n",
        "    pruner.score(valloader, num_batches=3)  # Fisher score stimato sui dati di validazione\n",
        "    masks = pruner.generate_masks(sparsity=0.2)\n",
        "\n",
        "    # 4. Ottimizzatore custom con maschera\n",
        "    optimizer = SparseSGDM(\n",
        "        model.head.parameters(),\n",
        "        lr=config[\"lr\"],\n",
        "        momentum=config[\"momentum\"],\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "        masks=masks\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "    # Struttura per salvare i risultati\n",
        "    results = {\n",
        "        'train_acc': [],\n",
        "        'train_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_loss': [],\n",
        "        'best_epoch': 0,\n",
        "        'best_val_acc': 0.0,\n",
        "        'test_acc': 0.0,\n",
        "        'test_loss': 0.0,\n",
        "        'config': config\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(20):\n",
        "        # Training loop (come prima)\n",
        "        # ...\n",
        "\n",
        "        # Dopo ogni epoca, salva le metriche\n",
        "        results['train_acc'].append(train_acc)\n",
        "        results['train_loss'].append(train_loss)\n",
        "        results['val_acc'].append(val_acc)\n",
        "        results['val_loss'].append(val_loss)\n",
        "\n",
        "        # Aggiorna il miglior modello\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            results['best_val_acc'] = best_val_acc\n",
        "            results['best_epoch'] = epoch\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "        # Report a Ray Tune (per monitorare durante il training)\n",
        "        report({\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"sparsity\": config[\"sparsity\"],\n",
        "            \"epoch\": epoch\n",
        "        })\n",
        "\n",
        "    # Dopo il training, valuta sul test set con il miglior modello\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.eval()\n",
        "    correct, total, test_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in testloader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "            test_loss += loss.item() * y.size(0)\n",
        "            _, pred = outputs.max(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    test_acc = correct / total\n",
        "    results['test_acc'] = test_acc\n",
        "    results['test_loss'] = test_loss / total\n",
        "\n",
        "    # Crea una checkpoint directory temporanea\n",
        "    checkpoint_dir = os.path.join(os.getcwd(), \"checkpoint\")\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Salva il modello e i risultati\n",
        "    torch.save(best_model_state, os.path.join(checkpoint_dir, \"model.pth\"))\n",
        "    with open(os.path.join(checkpoint_dir, \"results.json\"), \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    # Crea un checkpoint per Ray Tune\n",
        "    checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
        "\n",
        "    # Report finale con il checkpoint\n",
        "    report({\n",
        "        \"val_accuracy\": best_val_acc,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"test_accuracy\": test_acc,\n",
        "        \"sparsity\": config[\"sparsity\"],\n",
        "        \"checkpoint\": checkpoint\n",
        "    })\n",
        "\n",
        "    # Pulisci la directory temporanea\n",
        "    try:\n",
        "        shutil.rmtree(checkpoint_dir)\n",
        "    except:\n",
        "        pass\"\"\""
      ],
      "metadata": {
        "id": "ncfu0-9N0lLd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "518e3efd-5db8-40e1-c2b5-3aa78f6eba48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from ray.train import Checkpoint\\nimport json\\nimport os\\nimport torch\\nfrom ray.train import report\\nfrom torch.cuda.amp import autocast, GradScaler\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.data import DataLoader\\nimport shutil\\n\\ndef train_vit(config):\\n    # 1. Crea il modello\\n    model = create_dino_vit_s16_for_cifar100().to(DEVICE)\\n    criterion = nn.CrossEntropyLoss()\\n    scaler = GradScaler()\\n\\n    # 2. Prepara i dati\\n    pipeline = data_preprocessing.CIFAR100Pipeline(val_split=VAL_SPLIT, use_augment=True)\\n    trainset, valset, testset = pipeline.run_pipeline()\\n    trainloader = DataLoader(trainset, batch_size=config[\"batch_size\"], shuffle=True)\\n    valloader = DataLoader(valset, batch_size=config[\"batch_size\"])\\n    testloader = DataLoader(testset, batch_size=config[\"batch_size\"])\\n\\n    # 3. Calcola Fisher diagonal e maschera\\n    fisher_diag = compute_fisher_diag(model, valloader, criterion, DEVICE, num_batches=3)\\n    masks = fisher_mask_from_diag(fisher_diag, sparsity=config[\"sparsity\"])\\n\\n    # --- CALCOLO SCORE & MASCHERE CON IL TUO TALOSPRUNER ---\\n    pruner = TaLoSPruner(model.head, device=DEVICE)\\n    pruner.score(valloader, num_batches=3)  # Fisher score stimato sui dati di validazione\\n    masks = pruner.generate_masks(sparsity=0.2)\\n\\n    # 4. Ottimizzatore custom con maschera\\n    optimizer = SparseSGDM(\\n        model.head.parameters(),\\n        lr=config[\"lr\"],\\n        momentum=config[\"momentum\"],\\n        weight_decay=config[\"weight_decay\"],\\n        masks=masks\\n    )\\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\\n\\n    # Struttura per salvare i risultati\\n    results = {\\n        \\'train_acc\\': [],\\n        \\'train_loss\\': [],\\n        \\'val_acc\\': [],\\n        \\'val_loss\\': [],\\n        \\'best_epoch\\': 0,\\n        \\'best_val_acc\\': 0.0,\\n        \\'test_acc\\': 0.0,\\n        \\'test_loss\\': 0.0,\\n        \\'config\\': config\\n    }\\n\\n    best_val_acc = 0.0\\n    best_model_state = None\\n\\n    for epoch in range(20):\\n        # Training loop (come prima)\\n        # ...\\n\\n        # Dopo ogni epoca, salva le metriche\\n        results[\\'train_acc\\'].append(train_acc)\\n        results[\\'train_loss\\'].append(train_loss)\\n        results[\\'val_acc\\'].append(val_acc)\\n        results[\\'val_loss\\'].append(val_loss)\\n\\n        # Aggiorna il miglior modello\\n        if val_acc > best_val_acc:\\n            best_val_acc = val_acc\\n            results[\\'best_val_acc\\'] = best_val_acc\\n            results[\\'best_epoch\\'] = epoch\\n            best_model_state = model.state_dict()\\n\\n        # Report a Ray Tune (per monitorare durante il training)\\n        report({\\n            \"val_accuracy\": val_acc,\\n            \"train_accuracy\": train_acc,\\n            \"sparsity\": config[\"sparsity\"],\\n            \"epoch\": epoch\\n        })\\n\\n    # Dopo il training, valuta sul test set con il miglior modello\\n    model.load_state_dict(best_model_state)\\n    model.eval()\\n    correct, total, test_loss = 0, 0, 0.0\\n    with torch.no_grad():\\n        for x, y in testloader:\\n            x, y = x.to(DEVICE), y.to(DEVICE)\\n            outputs = model(x)\\n            loss = criterion(outputs, y)\\n            test_loss += loss.item() * y.size(0)\\n            _, pred = outputs.max(1)\\n            correct += (pred == y).sum().item()\\n            total += y.size(0)\\n\\n    test_acc = correct / total\\n    results[\\'test_acc\\'] = test_acc\\n    results[\\'test_loss\\'] = test_loss / total\\n\\n    # Crea una checkpoint directory temporanea\\n    checkpoint_dir = os.path.join(os.getcwd(), \"checkpoint\")\\n    os.makedirs(checkpoint_dir, exist_ok=True)\\n\\n    # Salva il modello e i risultati\\n    torch.save(best_model_state, os.path.join(checkpoint_dir, \"model.pth\"))\\n    with open(os.path.join(checkpoint_dir, \"results.json\"), \"w\") as f:\\n        json.dump(results, f, indent=4)\\n\\n    # Crea un checkpoint per Ray Tune\\n    checkpoint = Checkpoint.from_directory(checkpoint_dir)\\n\\n    # Report finale con il checkpoint\\n    report({\\n        \"val_accuracy\": best_val_acc,\\n        \"train_accuracy\": train_acc,\\n        \"test_accuracy\": test_acc,\\n        \"sparsity\": config[\"sparsity\"],\\n        \"checkpoint\": checkpoint\\n    })\\n\\n    # Pulisci la directory temporanea\\n    try:\\n        shutil.rmtree(checkpoint_dir)\\n    except:\\n        pass'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune.search.basic_variant import BasicVariantGenerator\n",
        "from ray.tune import CLIReporter\n",
        "import os\n",
        "\n",
        "storage_uri = f\"file://ray_results\"\n",
        "\n",
        "reporter = CLIReporter(\n",
        "    metric_columns=[\"val_accuracy\", \"train_accuracy\", \"test_accuracy\", \"sparsity\", \"training_iteration\"]\n",
        ")\n",
        "\n",
        "analysis = tune.run(\n",
        "    train_vit,\n",
        "    config=search_space,\n",
        "    storage_path=storage_uri,\n",
        "    search_alg=BasicVariantGenerator(),\n",
        "    num_samples=10,\n",
        "    resources_per_trial={\"cpu\": 2, \"gpu\": 1},\n",
        "    scheduler=ASHAScheduler(metric=\"val_accuracy\", mode=\"max\"),\n",
        "    name=\"vit_hyperparam_search\",\n",
        "    progress_reporter=reporter\n",
        ")"
      ],
      "metadata": {
        "id": "Q5Ai6dwh1O8Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "bd51d06f-7dd8-44f6-9193-1e57d20b9a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-127-d99af327ec26>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-127-d99af327ec26>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \"\"\"from ray import tune\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Ottieni il miglior trial\n",
        "best_trial = analysis.get_best_trial(\"val_accuracy\", mode=\"max\", scope=\"all\")\n",
        "\n",
        "# Percorso del checkpoint\n",
        "best_checkpoint = best_trial.checkpoint.value\n",
        "\n",
        "# Carica i risultati\n",
        "with open(os.path.join(best_checkpoint, \"results.json\"), \"r\") as f:\n",
        "    best_results = json.load(f)\n",
        "\n",
        "# Carica il modello\n",
        "best_model = create_dino_vit_s16_for_cifar100().to(DEVICE)\n",
        "best_model.load_state_dict(torch.load(os.path.join(best_checkpoint, \"model.pth\")))\"\"\""
      ],
      "metadata": {
        "id": "oKmQhj3wGOcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}