{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fb871c69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb871c69",
        "outputId": "c2f3eef0-3d7a-4fa5-8224-b08b06ab7d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch_optimizer) (2.6.0+cu124)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.2)\n",
            "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install timm torchvision\n",
        "!pip install wandb\n",
        "!pip install torch_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "54c3d9fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54c3d9fd",
        "outputId": "f525e125-25e3-4138-9bda-d5fedae8299c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch_optimizer import Ranger\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import timm\n",
        "from timm.loss import LabelSmoothingCrossEntropy\n",
        "from timm.models.layers import DropPath\n",
        "import data_preprocessing\n",
        "from wandb_logger import WandBLogger\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import editing\n",
        "from editing import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "12b841a9",
      "metadata": {
        "id": "12b841a9"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9b74cad4",
      "metadata": {
        "id": "9b74cad4"
      },
      "outputs": [],
      "source": [
        "# THE WHOLE TRAINING SETTINGS ARE HERE!\n",
        "\n",
        "# GENERAL\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 30\n",
        "VAL_SPLIT = 0.1\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BACKBONE_FREEZING = True\n",
        "\n",
        "# LOSS\n",
        "SMOOTHING=0.1\n",
        "# OPTIMIZER\n",
        "OPTIMIZER_NAME = \"ssgd\"\n",
        "LR=0.001\n",
        "BETAS=(0.9, 0.999)\n",
        "WEIGHT_DECAY=0.05\n",
        "WARMUP_EPOCHS=5\n",
        "NESTEROV=False\n",
        "MOMENTUM=0.9\n",
        "EPSILON = 1e-8\n",
        "# SCHEDULER\n",
        "SCHEDULER_NAME = \"CosineAnnealingWarmRestarts\"\n",
        "START_FACTOR=1e-6 / 5e-5\n",
        "T_MAX=EPOCHS - WARMUP_EPOCHS\n",
        "# Early Stopping\n",
        "PATIENCE = 6\n",
        "\n",
        "#TALOS CONFIG\n",
        "FINAL_SPARSITY = 0.6\n",
        "NUM_BATCHES = 4\n",
        "ROUNDS = 4\n",
        "CALIBRATION_MODE = \"least_sensitive\" # least_sensitive or most_sensitive\n",
        "MODE = \"full\" # full if want to include all layers, head for head only, pfededit for custom topk client selection\n",
        "K = 3 #pfedit setting\n",
        "FACTOR = 0.5 #probability of not doing pfededit\n",
        "\n",
        "TALOS_CONFIG = {\n",
        "    \"final_sparsity\": FINAL_SPARSITY,\n",
        "    \"num_batches\": NUM_BATCHES,\n",
        "    \"rounds\": ROUNDS,\n",
        "    \"calibration_mode\": CALIBRATION_MODE,\n",
        "    \"mode\": MODE,\n",
        "    \"k\": K,\n",
        "    \"factor\": FACTOR\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e47d1260",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e47d1260",
        "outputId": "a46e0a48-8e03-4474-92a9-851b892bde6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login 89e5fee022a3a1cf86f958ee0b3dff6f2aa57aad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4bd9739e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bd9739e",
        "outputId": "70d59c68-b512-4248-ca7a-f8be24a8da20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:17<00:00, 9.72MB/s]\n"
          ]
        }
      ],
      "source": [
        "pipeline = data_preprocessing.CIFAR100Pipeline(val_split=VAL_SPLIT, use_augment=True)\n",
        "trainset, valset, testset = pipeline.run_pipeline()\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=BATCH_SIZE)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "85ba1bde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "36bcf31a82f6410aa31d75ae42f322af",
            "2290cd8428cb4d09a43aad160458cd53",
            "62ad17d4d6ec48cab11bd34746ebc313",
            "a2b4f27d04d04e6793be4ab4e090351c",
            "8101cb0c8d584eacbd3b0ed9034833a9",
            "7aec4c8cd9394bbb9bcc2c068d4ff852",
            "622e48756bbe4313b7de48a5f8a50d65",
            "54e6cdad17e64a91a069b6602c724a87",
            "bd4af89ba1e44f5692dde0c2107bc914",
            "9e2f927f477540dba1b9426eb0ace4db",
            "54b66d6413934fe583ede5a8c92dba35"
          ]
        },
        "id": "85ba1bde",
        "outputId": "a9df3088-fdf8-4675-e371-2c2555ce60b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name vit_small_patch16_224_dino to current vit_small_patch16_224.dino.\n",
            "  model = create_fn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/86.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36bcf31a82f6410aa31d75ae42f322af"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create model\n",
        "def create_dino_vit_s16_for_cifar100(freezing=BACKBONE_FREEZING):\n",
        "    model = timm.create_model(\"vit_small_patch16_224_dino\", pretrained=True, num_classes=0)\n",
        "\n",
        "    # Replace the head with CIFAR-100 classification head\n",
        "    model.head = nn.Linear(model.num_features, 100)\n",
        "\n",
        "    if freezing:\n",
        "      # Freeze all parameters except head\n",
        "      for param in model.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "      # Unfreeze only the head\n",
        "      for param in model.head.parameters():\n",
        "          param.requires_grad = True\n",
        "    else:\n",
        "\n",
        "      for param in model.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "      for param in model.head.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_dino_vit_s16_for_cifar100(BACKBONE_FREEZING).to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bf460878",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf460878",
        "outputId": "9d57bda8-94e0-41fd-bf1d-b3961fb6a17b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟢 Pruning will be applied to the entire model.\n",
            "🔎 Starting multi-round calibration for mode 'full'.\n",
            "🌀 Calibration Round 1/4\n",
            "📝 Calculating Fisher Information on 4 batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/editing.py:189: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fisher Information Computation Completed.\n",
            "🌀 Calibration Round 2/4\n",
            "📝 Calculating Fisher Information on 4 batches...\n",
            "✅ Fisher Information Computation Completed.\n",
            "🌀 Calibration Round 3/4\n",
            "📝 Calculating Fisher Information on 4 batches...\n",
            "✅ Fisher Information Computation Completed.\n",
            "🌀 Calibration Round 4/4\n",
            "📝 Calculating Fisher Information on 4 batches...\n",
            "✅ Fisher Information Computation Completed.\n",
            "✅ Mask Calibration Completed!\n"
          ]
        }
      ],
      "source": [
        "def build_pruner():\n",
        "    pruner = TaLoSPruner(\n",
        "        model = model,\n",
        "        final_sparsity=FINAL_SPARSITY,\n",
        "        device = DEVICE,\n",
        "        num_batches=NUM_BATCHES,\n",
        "        rounds=ROUNDS,\n",
        "        mode=MODE\n",
        "    )\n",
        "    return pruner\n",
        "\n",
        "pruner = build_pruner()\n",
        "pruner.calibrate_masks(trainloader, strategy=CALIBRATION_MODE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e60d0902",
      "metadata": {
        "id": "e60d0902"
      },
      "outputs": [],
      "source": [
        "def build_optimizer(optimizer_name, model, masks=None):\n",
        "    if optimizer_name.lower() == \"ssgd\":\n",
        "        config = {\n",
        "            \"lr\": LR,\n",
        "            \"momentum\": MOMENTUM,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "        }\n",
        "        optimizer = editing.SparseSGDM(\n",
        "            model.parameters(),\n",
        "            lr=LR,\n",
        "            momentum=MOMENTUM,\n",
        "            weight_decay=WEIGHT_DECAY,\n",
        "            masks=pruner.masks\n",
        "        )\n",
        "    elif optimizer_name.lower() == \"sparseadamw\":\n",
        "        config = {\n",
        "            \"lr\": LR,\n",
        "            \"betas\": BETAS,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"eps\": EPSILON\n",
        "\n",
        "        }\n",
        "        optimizer = editing.SparseAdamW(\n",
        "            model.parameters(),\n",
        "            lr=LR,\n",
        "            betas=BETAS,\n",
        "            weight_decay=WEIGHT_DECAY,\n",
        "            eps=EPSILON,\n",
        "            masks=pruner.masks\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "    print(f\"Optimizer '{optimizer_name}' initialized successfully.\")\n",
        "    return config, optimizer\n",
        "\n",
        "def build_scheduler(scheduler_name, optimizer):\n",
        "    if scheduler_name.lower() == \"ranger + onecyclelr\":\n",
        "        config = {\n",
        "            \"max_lr\": LR,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"steps_per_epoch\": len(trainloader),\n",
        "            \"pct_start\": 0.3,\n",
        "            \"anneal_strategy\": \"cos\",\n",
        "            \"div_factor\": 25.0,\n",
        "            \"final_div_factor\": 1e4\n",
        "        }\n",
        "        scheduler = OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=LR,\n",
        "            epochs=EPOCHS,\n",
        "            steps_per_epoch=len(trainloader),\n",
        "            pct_start=0.3,\n",
        "            anneal_strategy=\"cos\",\n",
        "            div_factor=25.0,\n",
        "            final_div_factor=1e4\n",
        "        )\n",
        "    elif scheduler_name.lower() == \"cosineannealinglr + warmup\":\n",
        "        config = {\n",
        "            \"warmup_epochs\": WARMUP_EPOCHS,\n",
        "            \"start_factor\": START_FACTOR,\n",
        "            \"t_max\": T_MAX,\n",
        "            \"milestones\": [WARMUP_EPOCHS]\n",
        "        }\n",
        "        scheduler = optim.lr_scheduler.SequentialLR(\n",
        "            optimizer,\n",
        "            schedulers=[\n",
        "                optim.lr_scheduler.LinearLR(optimizer, start_factor=START_FACTOR, total_iters=WARMUP_EPOCHS),\n",
        "                optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX)\n",
        "            ],\n",
        "            milestones=[WARMUP_EPOCHS]\n",
        "        )\n",
        "    elif scheduler_name.lower() == \"cosineannealingwarmrestarts\":\n",
        "        config = {\n",
        "            \"T_0\": 10,\n",
        "            \"T_mult\": 2\n",
        "        }\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=10,\n",
        "            T_mult=2\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported scheduler: {scheduler_name}\")\n",
        "\n",
        "    print(f\"Scheduler '{scheduler_name}' initialized successfully.\")\n",
        "    return config, scheduler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "378fc75e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "378fc75e",
        "outputId": "b7567a41-018f-45c5-faee-1ffe9b39a5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Mapping parameters to their masks...\n",
            "✅ Mapped 150 parameters to masks.\n",
            "Optimizer 'ssgd' initialized successfully.\n",
            "Scheduler 'CosineAnnealingWarmRestarts' initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "OPTIMIZER_CONFIG, optimizer = build_optimizer(OPTIMIZER_NAME, model)\n",
        "SCHEDULER_CONFIG, scheduler = build_scheduler(SCHEDULER_NAME, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dc3b04a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc3b04a5",
        "outputId": "eb49dc4a-be46-49a2-a916-94bdfbe4c7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Trainable params: 38,500 / 21,704,164\n"
          ]
        }
      ],
      "source": [
        "print(next(model.parameters()).device)\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {trainable:,} / {total:,}\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "99cb535f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "99cb535f",
        "outputId": "9b8a1197-7b79-4ec6-cf02-b52b4fa0c9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms348517giuseppe\u001b[0m (\u001b[33ms348517giuseppe-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250516_090522-1ta3e64j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/polito-fl/federated-learning-project/runs/1ta3e64j' target=\"_blank\">CENTRALIZED_TALOS_20250516-090519</a></strong> to <a href='https://wandb.ai/polito-fl/federated-learning-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/polito-fl/federated-learning-project' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/polito-fl/federated-learning-project/runs/1ta3e64j' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project/runs/1ta3e64j</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run_name = f\"CENTRALIZED_TALOS_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "logger = WandBLogger(\n",
        "    project_name=\"federated-learning-project\",\n",
        "    run_name=run_name,\n",
        "    config={\n",
        "        # Training Setup\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"device\": DEVICE,\n",
        "        \"backbone_freezing\": BACKBONE_FREEZING,\n",
        "\n",
        "        # Loss Configuration\n",
        "        \"label_smoothing\": SMOOTHING,\n",
        "\n",
        "        # Optimizer Configuration\n",
        "        \"optimizer\": OPTIMIZER_NAME,\n",
        "        \"optimizer_config\": OPTIMIZER_CONFIG,\n",
        "\n",
        "        # Scheduler Configuration\n",
        "        \"scheduler\": SCHEDULER_NAME,\n",
        "        \"scheduler_config\": SCHEDULER_CONFIG,\n",
        "\n",
        "        # Talos Configuration\n",
        "        \"talos_config\": TALOS_CONFIG,\n",
        "\n",
        "        # Early Stopping\n",
        "        \"patience\": PATIENCE\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c5f853",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23c5f853",
        "outputId": "9245eb66-02ca-4ee1-e0df-7cc7459f0598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-b648f64fadd1>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-13-b648f64fadd1>:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/30 — Train Acc: 0.1161 | Val Acc: 0.2398\n",
            "Epoch 02/30 — Train Acc: 0.3256 | Val Acc: 0.3840\n",
            "Epoch 03/30 — Train Acc: 0.4307 | Val Acc: 0.4502\n",
            "Epoch 04/30 — Train Acc: 0.4864 | Val Acc: 0.4890\n",
            "Epoch 05/30 — Train Acc: 0.5206 | Val Acc: 0.5090\n",
            "Epoch 06/30 — Train Acc: 0.5418 | Val Acc: 0.5232\n",
            "Epoch 07/30 — Train Acc: 0.5542 | Val Acc: 0.5344\n",
            "Epoch 08/30 — Train Acc: 0.5627 | Val Acc: 0.5388\n",
            "Epoch 09/30 — Train Acc: 0.5676 | Val Acc: 0.5408\n",
            "Epoch 10/30 — Train Acc: 0.5691 | Val Acc: 0.5414\n",
            "Epoch 11/30 — Train Acc: 0.5711 | Val Acc: 0.5586\n",
            "Epoch 12/30 — Train Acc: 0.5859 | Val Acc: 0.5730\n",
            "Epoch 13/30 — Train Acc: 0.5967 | Val Acc: 0.5782\n",
            "Epoch 14/30 — Train Acc: 0.6051 | Val Acc: 0.5832\n",
            "Epoch 15/30 — Train Acc: 0.6104 | Val Acc: 0.5902\n",
            "Epoch 16/30 — Train Acc: 0.6140 | Val Acc: 0.5940\n",
            "Epoch 17/30 — Train Acc: 0.6175 | Val Acc: 0.5948\n",
            "Epoch 18/30 — Train Acc: 0.6209 | Val Acc: 0.5974\n",
            "Epoch 19/30 — Train Acc: 0.6229 | Val Acc: 0.5986\n",
            "Epoch 20/30 — Train Acc: 0.6248 | Val Acc: 0.5986\n",
            "Epoch 21/30 — Train Acc: 0.6254 | Val Acc: 0.5992\n",
            "Epoch 22/30 — Train Acc: 0.6276 | Val Acc: 0.6012\n",
            "Epoch 23/30 — Train Acc: 0.6291 | Val Acc: 0.6018\n",
            "Epoch 24/30 — Train Acc: 0.6292 | Val Acc: 0.5998\n",
            "Epoch 25/30 — Train Acc: 0.6299 | Val Acc: 0.6024\n"
          ]
        }
      ],
      "source": [
        "# Loss with label smoothing\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=SMOOTHING)\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "best_val_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    correct, total, train_loss = 0, 0, 0.0\n",
        "\n",
        "    for x, y in trainloader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer) #Optimizer applies the mask!!!\n",
        "        scaler.update()\n",
        "\n",
        "        # scheduler.step()\n",
        "\n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_acc = correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    logger.log_metrics({\n",
        "      \"train_loss\": train_loss,\n",
        "      \"train_acc\": train_acc,\n",
        "      \"learning_rate\": scheduler.get_last_lr()[0]\n",
        "    }, step=epoch)\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total, val_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in valloader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            val_loss += loss.item() * y.size(0)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    val_acc = correct / total\n",
        "    val_loss /= total\n",
        "    logger.log_metrics({\n",
        "      \"val_loss\": val_loss,\n",
        "      \"val_acc\": val_acc\n",
        "    }, step=epoch)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{EPOCHS} — Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        best_model_state = model.state_dict()  # save best model\n",
        "        logger.log_model(model, path=\"best_model.pth\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(best_model_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb5f60e9",
      "metadata": {
        "id": "bb5f60e9"
      },
      "outputs": [],
      "source": [
        "#  test\n",
        "model.eval()\n",
        "correct, total, test_loss = 0, 0, 0.0\n",
        "with torch.no_grad():\n",
        "    for x, y in testloader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "\n",
        "        test_loss += loss.item() * y.size(0)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "test_acc = correct / total\n",
        "test_loss /= total\n",
        "\n",
        "logger.log_metrics({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_acc\": test_acc\n",
        "})\n",
        "\n",
        "logger.finish()\n",
        "\n",
        "print(f\"\\n Final Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "36bcf31a82f6410aa31d75ae42f322af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2290cd8428cb4d09a43aad160458cd53",
              "IPY_MODEL_62ad17d4d6ec48cab11bd34746ebc313",
              "IPY_MODEL_a2b4f27d04d04e6793be4ab4e090351c"
            ],
            "layout": "IPY_MODEL_8101cb0c8d584eacbd3b0ed9034833a9"
          }
        },
        "2290cd8428cb4d09a43aad160458cd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aec4c8cd9394bbb9bcc2c068d4ff852",
            "placeholder": "​",
            "style": "IPY_MODEL_622e48756bbe4313b7de48a5f8a50d65",
            "value": "model.safetensors: 100%"
          }
        },
        "62ad17d4d6ec48cab11bd34746ebc313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e6cdad17e64a91a069b6602c724a87",
            "max": 86676340,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd4af89ba1e44f5692dde0c2107bc914",
            "value": 86676340
          }
        },
        "a2b4f27d04d04e6793be4ab4e090351c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e2f927f477540dba1b9426eb0ace4db",
            "placeholder": "​",
            "style": "IPY_MODEL_54b66d6413934fe583ede5a8c92dba35",
            "value": " 86.7M/86.7M [00:00&lt;00:00, 352MB/s]"
          }
        },
        "8101cb0c8d584eacbd3b0ed9034833a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aec4c8cd9394bbb9bcc2c068d4ff852": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "622e48756bbe4313b7de48a5f8a50d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54e6cdad17e64a91a069b6602c724a87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd4af89ba1e44f5692dde0c2107bc914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e2f927f477540dba1b9426eb0ace4db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54b66d6413934fe583ede5a8c92dba35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}