{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "vXKl1PIYUlvb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXKl1PIYUlvb",
        "outputId": "7ce68ad4-0d3e-4b59-80d8-13bde781fbfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.0/540.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.3 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: flwr[simulation] in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (44.0.3)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.72.1)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.0.2)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (4.25.8)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (3.23.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Collecting ray==2.31.0 (from flwr[simulation])\n",
            "  Downloading ray-2.31.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (8.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (4.24.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (24.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.19.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (4.14.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.25.1)\n",
            "Downloading ray-2.31.0-cp311-cp311-manylinux2014_x86_64.whl (66.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ray\n",
            "Successfully installed ray-2.31.0\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.32.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr torch torchvision tensorboard\n",
        "!pip install -U \"flwr[simulation]\"\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fb871c69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb871c69",
        "outputId": "50d3b024-4920-4cce-aa3a-5806d4528a47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.32.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.8)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch_optimizer) (2.6.0+cu124)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.2)\n",
            "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install timm torchvision\n",
        "!pip install wandb\n",
        "!pip install torch_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "54c3d9fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54c3d9fd",
        "outputId": "f8f46e70-5b9d-4985-93f9-80ababa70dbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch_optimizer import Ranger\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import timm\n",
        "from timm.loss import LabelSmoothingCrossEntropy\n",
        "from timm.models.layers import DropPath\n",
        "import data_preprocessing\n",
        "from wandb_logger import WandBLogger\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import editing\n",
        "from editing import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "12b841a9",
      "metadata": {
        "id": "12b841a9"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "9b74cad4",
      "metadata": {
        "id": "9b74cad4"
      },
      "outputs": [],
      "source": [
        "# THE WHOLE TRAINING SETTINGS ARE HERE!\n",
        "\n",
        "# GENERAL\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 15\n",
        "VAL_SPLIT = 0.1\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BACKBONE_FREEZING = False\n",
        "\n",
        "# LOSS\n",
        "SMOOTHING=0.1\n",
        "# OPTIMIZER\n",
        "OPTIMIZER_NAME = \"sadamw\"\n",
        "LR=0.00005\n",
        "BETAS=(0.9, 0.999)\n",
        "WEIGHT_DECAY=0.05\n",
        "WARMUP_EPOCHS=5\n",
        "NESTEROV=False\n",
        "MOMENTUM=0.9\n",
        "EPSILON = 1e-8\n",
        "# SCHEDULER\n",
        "SCHEDULER_NAME = \"CosineAnnealingWarmRestarts\"\n",
        "START_FACTOR=1e-6 / 5e-5\n",
        "T_MAX=EPOCHS - WARMUP_EPOCHS\n",
        "# Early Stopping\n",
        "PATIENCE = 6\n",
        "\n",
        "#TALOS CONFIG\n",
        "FINAL_SPARSITY = 0.6\n",
        "NUM_BATCHES = 4\n",
        "ROUNDS = 4\n",
        "CALIBRATION_MODE = \"most_sensitive\" # least_sensitive or most_sensitive\n",
        "MODE = \"full\" # full if want to include all layers, head for head only, pfededit for custom topk client selection\n",
        "K = 3 #pfedit setting\n",
        "FACTOR = 0.5 #probability of not doing pfededit\n",
        "\n",
        "TALOS_CONFIG = {\n",
        "    \"final_sparsity\": FINAL_SPARSITY,\n",
        "    \"num_batches\": NUM_BATCHES,\n",
        "    \"rounds\": ROUNDS,\n",
        "    \"calibration_mode\": CALIBRATION_MODE,\n",
        "    \"mode\": MODE,\n",
        "    \"k\": K,\n",
        "    \"factor\": FACTOR\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e47d1260",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e47d1260",
        "outputId": "ec0aedec-b310-4210-d963-ef8fd8fadc7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login a8e9e7fc10d033043f95b8b53c354022103a54e7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "4bd9739e",
      "metadata": {
        "id": "4bd9739e"
      },
      "outputs": [],
      "source": [
        "pipeline = data_preprocessing.CIFAR100Pipeline(val_split=VAL_SPLIT, use_augment=True)\n",
        "trainset, valset, testset = pipeline.run_pipeline()\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=BATCH_SIZE)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "85ba1bde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85ba1bde",
        "outputId": "d4eb8aca-0c01-4399-eae2-8768d643e1ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name vit_small_patch16_224_dino to current vit_small_patch16_224.dino.\n",
            "  model = create_fn(\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "def create_dino_vit_s16_for_cifar100(freezing=BACKBONE_FREEZING):\n",
        "    model = timm.create_model(\"vit_small_patch16_224_dino\", pretrained=True, num_classes=0)\n",
        "\n",
        "    # Replace the head with CIFAR-100 classification head\n",
        "    model.head = nn.Linear(model.num_features, 100)\n",
        "\n",
        "    if freezing:\n",
        "      # Freeze all parameters except head\n",
        "      for param in model.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "      # Unfreeze only the head\n",
        "      for param in model.head.parameters():\n",
        "          param.requires_grad = True\n",
        "    else:\n",
        "\n",
        "      for param in model.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "      for param in model.head.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_dino_vit_s16_for_cifar100(BACKBONE_FREEZING).to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "bf460878",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf460878",
        "outputId": "cf4930ac-5447-4182-ce5e-3709bcb1a8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟢 Pruning will be applied to the entire model.\n",
            "🔎 Starting multi-round calibration for mode 'full'.\n",
            "🌀 Calibration Round 1/4\n",
            "📝 Calculating Fisher Information on 4 batches...\n",
            "✅ Fisher Information Computation Completed.\n",
            "🌀 Calibration Round 2/4\n",
            "📝 Calculating Fisher Information on 4 batches...\n",
            "✅ Fisher Information Computation Completed.\n",
            "🌀 Calibration Round 3/4\n",
            "📝 Calculating Fisher Information on 4 batches...\n",
            "✅ Fisher Information Computation Completed.\n",
            "🌀 Calibration Round 4/4\n",
            "📝 Calculating Fisher Information on 4 batches...\n",
            "✅ Fisher Information Computation Completed.\n",
            "✅ Mask Calibration Completed!\n"
          ]
        }
      ],
      "source": [
        "def build_pruner():\n",
        "    pruner = TaLoSPruner(\n",
        "        model = model,\n",
        "        final_sparsity=FINAL_SPARSITY,\n",
        "        device = DEVICE,\n",
        "        num_batches=NUM_BATCHES,\n",
        "        rounds=ROUNDS,\n",
        "        mode=MODE\n",
        "    )\n",
        "    return pruner\n",
        "\n",
        "pruner = build_pruner()\n",
        "pruner.calibrate_masks(trainloader, strategy=CALIBRATION_MODE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e60d0902",
      "metadata": {
        "id": "e60d0902"
      },
      "outputs": [],
      "source": [
        "def build_optimizer(optimizer_name, model, masks=None):\n",
        "    if optimizer_name.lower() == \"ssgd\":\n",
        "        config = {\n",
        "            \"lr\": LR,\n",
        "            \"momentum\": MOMENTUM,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "        }\n",
        "        optimizer = editing.SparseSGDM(\n",
        "            model.parameters(),\n",
        "            lr=LR,\n",
        "            momentum=MOMENTUM,\n",
        "            weight_decay=WEIGHT_DECAY,\n",
        "            masks=pruner.masks\n",
        "        )\n",
        "    elif optimizer_name.lower() == \"sadamw\":\n",
        "        config = {\n",
        "            \"lr\": LR,\n",
        "            \"betas\": BETAS,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"eps\": EPSILON\n",
        "\n",
        "        }\n",
        "        optimizer = editing.SparseAdamW(\n",
        "            model.parameters(),\n",
        "            lr=LR,\n",
        "            betas=BETAS,\n",
        "            weight_decay=WEIGHT_DECAY,\n",
        "            eps=EPSILON,\n",
        "            masks=pruner.masks\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "    print(f\"Optimizer '{optimizer_name}' initialized successfully.\")\n",
        "    return config, optimizer\n",
        "\n",
        "def build_scheduler(scheduler_name, optimizer):\n",
        "    if scheduler_name.lower() == \"ranger + onecyclelr\":\n",
        "        config = {\n",
        "            \"max_lr\": LR,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"steps_per_epoch\": len(trainloader),\n",
        "            \"pct_start\": 0.3,\n",
        "            \"anneal_strategy\": \"cos\",\n",
        "            \"div_factor\": 25.0,\n",
        "            \"final_div_factor\": 1e4\n",
        "        }\n",
        "        scheduler = OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=LR,\n",
        "            epochs=EPOCHS,\n",
        "            steps_per_epoch=len(trainloader),\n",
        "            pct_start=0.3,\n",
        "            anneal_strategy=\"cos\",\n",
        "            div_factor=25.0,\n",
        "            final_div_factor=1e4\n",
        "        )\n",
        "    elif scheduler_name.lower() == \"cosineannealinglr + warmup\":\n",
        "        config = {\n",
        "            \"warmup_epochs\": WARMUP_EPOCHS,\n",
        "            \"start_factor\": START_FACTOR,\n",
        "            \"t_max\": T_MAX,\n",
        "            \"milestones\": [WARMUP_EPOCHS]\n",
        "        }\n",
        "        scheduler = optim.lr_scheduler.SequentialLR(\n",
        "            optimizer,\n",
        "            schedulers=[\n",
        "                optim.lr_scheduler.LinearLR(optimizer, start_factor=START_FACTOR, total_iters=WARMUP_EPOCHS),\n",
        "                optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX)\n",
        "            ],\n",
        "            milestones=[WARMUP_EPOCHS]\n",
        "        )\n",
        "    elif scheduler_name.lower() == \"cosineannealingwarmrestarts\":\n",
        "        config = {\n",
        "            \"T_0\": 10,\n",
        "            \"T_mult\": 2\n",
        "        }\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=10,\n",
        "            T_mult=2\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported scheduler: {scheduler_name}\")\n",
        "\n",
        "    print(f\"Scheduler '{scheduler_name}' initialized successfully.\")\n",
        "    return config, scheduler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "378fc75e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "378fc75e",
        "outputId": "853c2c85-fc4d-4ca4-a65f-f8fc925e5764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Mapping parameters to their masks...\n",
            "✅ Mapped 150 parameters to masks.\n",
            "Optimizer 'sadamw' initialized successfully.\n",
            "Scheduler 'CosineAnnealingWarmRestarts' initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "OPTIMIZER_CONFIG, optimizer = build_optimizer(OPTIMIZER_NAME, model)\n",
        "SCHEDULER_CONFIG, scheduler = build_scheduler(SCHEDULER_NAME, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "dc3b04a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc3b04a5",
        "outputId": "95c83141-4c9e-4016-c0a3-6bdf2e2d148d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Trainable params: 21,704,164 / 21,704,164\n"
          ]
        }
      ],
      "source": [
        "print(next(model.parameters()).device)\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {trainable:,} / {total:,}\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "99cb535f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "99cb535f",
        "outputId": "0696d42c-329f-4def-9659-57871e8bef1c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_140445-5pjkl3s7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/polito-fl/federated-learning-project/runs/5pjkl3s7' target=\"_blank\">CENTRALIZED_TALOS_20250616-140445</a></strong> to <a href='https://wandb.ai/polito-fl/federated-learning-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/polito-fl/federated-learning-project' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/polito-fl/federated-learning-project/runs/5pjkl3s7' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project/runs/5pjkl3s7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run_name = f\"CENTRALIZED_TALOS_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "logger = WandBLogger(\n",
        "    project_name=\"federated-learning-project\",\n",
        "    run_name=run_name,\n",
        "    config={\n",
        "        # Training Setup\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"device\": DEVICE,\n",
        "        \"backbone_freezing\": BACKBONE_FREEZING,\n",
        "\n",
        "        # Loss Configuration\n",
        "        \"label_smoothing\": SMOOTHING,\n",
        "\n",
        "        # Optimizer Configuration\n",
        "        \"optimizer\": OPTIMIZER_NAME,\n",
        "        \"optimizer_config\": OPTIMIZER_CONFIG,\n",
        "\n",
        "        # Scheduler Configuration\n",
        "        \"scheduler\": SCHEDULER_NAME,\n",
        "        \"scheduler_config\": SCHEDULER_CONFIG,\n",
        "\n",
        "        # Talos Configuration\n",
        "        \"talos_config\": TALOS_CONFIG,\n",
        "\n",
        "        # Early Stopping\n",
        "        \"patience\": PATIENCE\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c5f853",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23c5f853",
        "outputId": "edcc4155-70cc-421b-ba7f-2db5c1b6b964"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-41-556336675>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-41-556336675>:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        }
      ],
      "source": [
        "# Loss with label smoothing\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=SMOOTHING)\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "best_val_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    correct, total, train_loss = 0, 0, 0.0\n",
        "\n",
        "    for x, y in trainloader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer) #Optimizer applies the mask!!!\n",
        "        scaler.update()\n",
        "\n",
        "        # scheduler.step()\n",
        "\n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_acc = correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    logger.log_metrics({\n",
        "      \"train_loss\": train_loss,\n",
        "      \"train_acc\": train_acc,\n",
        "      \"learning_rate\": scheduler.get_last_lr()[0]\n",
        "    }, step=epoch)\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total, val_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in valloader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            val_loss += loss.item() * y.size(0)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    val_acc = correct / total\n",
        "    val_loss /= total\n",
        "    logger.log_metrics({\n",
        "      \"val_loss\": val_loss,\n",
        "      \"val_acc\": val_acc\n",
        "    }, step=epoch)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{EPOCHS} — Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        best_model_state = model.state_dict()  # save best model\n",
        "        logger.log_model(model, path=\"best_model.pth\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(best_model_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "bb5f60e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "bb5f60e9",
        "outputId": "bf9fd7f1-9f73-436c-f9c2-c36cf78d2c90"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>█▇▇▆▄▃▂▂▁████▇▇</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▇████████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▇▇▇█████▆▇███</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▂▂▂▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>4e-05</td></tr><tr><td>test_acc</td><td>0.8639</td></tr><tr><td>test_loss</td><td>1.26236</td></tr><tr><td>train_acc</td><td>0.99951</td></tr><tr><td>train_loss</td><td>0.8096</td></tr><tr><td>val_acc</td><td>0.868</td></tr><tr><td>val_loss</td><td>1.25806</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CENTRALIZED_TALOS_20250616-131427</strong> at: <a href='https://wandb.ai/polito-fl/federated-learning-project/runs/mjoq10lt' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project/runs/mjoq10lt</a><br> View project at: <a href='https://wandb.ai/polito-fl/federated-learning-project' target=\"_blank\">https://wandb.ai/polito-fl/federated-learning-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_131428-mjoq10lt/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Final Test Accuracy: 0.8639 | Test Loss: 1.2624\n"
          ]
        }
      ],
      "source": [
        "#  test\n",
        "model.eval()\n",
        "correct, total, test_loss = 0, 0, 0.0\n",
        "with torch.no_grad():\n",
        "    for x, y in testloader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "\n",
        "        test_loss += loss.item() * y.size(0)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "test_acc = correct / total\n",
        "test_loss /= total\n",
        "\n",
        "logger.log_metrics({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_acc\": test_acc\n",
        "})\n",
        "\n",
        "logger.finish()\n",
        "\n",
        "print(f\"\\n Final Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
